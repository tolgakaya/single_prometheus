# Stage 2 Token Overflow Root Cause Analysis

## Problem

**Hata**: 263,495 tokens (106% over 128K limit)
**Yer**: Stage 2 Pattern Analysis AI Agent

## Root Cause Discovery

### Data Flow Analysis

1. **Node 9 Output**: 17,587 bytes (17KB JSON)
2. **Stage 2 Prompt**: 4,411 bytes (4KB text)
3. **AgentTools (HTTP queries)**: 5 queries Ã— 12 namespaces Ã— 1 hour = **MASSIVE log data**

**Token explosion**: 17KB input + 4KB prompt â†’ **263,495 tokens** = HTTP tool responses!

### Critical Finding: Unlimited Queries in AgentTools.txt

**Stage 2 kullandÄ±ÄŸÄ± 5 HTTP tool**:

#### Query 1: Thread Correlation Analyzer âœ… (SÄ±nÄ±rlÄ±)
```logql
sum by (process_thread_name, service_name, namespace, error_type) (
  count_over_time(
    {namespace=~"12_namespaces"}
    | json
    | process_thread_name=~"taskScheduler-.*|Thread-.*"
    | log_level="ERROR"
    [1h]  â† 1 saatlik aggregation
  )
)
```
**Token impact**: Aggregated count â†’ **KÃœÃ‡ÃœK** (sadece sayÄ±sal sonuÃ§lar)

#### Query 2: Service Error Distribution âœ… (SÄ±nÄ±rlÄ±)
```logql
sum(rate({namespace=~"12_namespaces"} |~ "error" [1m])) by (service_name,pod,namespace)
```
**Token impact**: Rate over 1m aggregated â†’ **KÃœÃ‡ÃœK** (sayÄ±sal metric)

#### Query 3: Cascade Timeline Reconstructor âŒ **SÄ°NIRSIZ!**
```logql
{namespace=~"12_namespaces"}
| json
| __error__=""
| log_level=~"ERROR|FATAL|WARN"
| line_format "{{.timestamp}}|{{.namespace}}|{{.service_name}}|{{.process_thread_name}}|{{.error_type}}|{{.message}}"
```

**SORUN**:
- âŒ **NO time range specified** ([1h] yok!)
- âŒ **NO topk/limit** (tÃ¼m loglarÄ± dÃ¶ndÃ¼rÃ¼yor!)
- âŒ **NO count_over_time** (raw logs!)

**n8n behavior**: Default time range = START_TIME to END_TIME (1 HOUR!)

**Token impact**:
- 12 namespaces
- 1 hour of logs
- ERROR + FATAL + WARN levels
- **Estimated**: 50,000-100,000 log lines Ã— 100 tokens/line = **5M-10M tokens!** ğŸ”´

#### Query 4: Request ID Correlation âŒ **SÄ°NIRSIZ!**
```logql
{namespace=~"12_namespaces"}
|~ "trace_id|traceID|correlation_id"
|~ "error"
```

**SORUN**:
- âŒ **NO time range** ([1h] yok!)
- âŒ **NO topk/limit**
- âŒ **NO aggregation**

**Token impact**:
- TÃ¼m trace_id iÃ§eren error loglarÄ±
- 1 saat
- **Estimated**: 10,000-50,000 log lines Ã— 100 tokens/line = **1M-5M tokens!** ğŸ”´

#### Query 5: Error Pattern Analyzer âš ï¸ (KÄ±smi SÄ±nÄ±rlÄ±)
```logql
topk(20, sum by (error, namespace) (
  count_over_time(
    {namespace=~"12_namespaces"}
    |~ "error"
    | json
    | error!=""
    [5m]  â† 5 dakikalÄ±k aggregation
  )
))
```
**Token impact**: topk(20) + count_over_time â†’ **ORTA** (20 error Ã— namespace data)

---

## Token Hesaplama

### GerÃ§ek Data
```
Input JSON: 17KB
Prompt: 4KB
Total input: 21KB â‰ˆ 5,250 tokens
```

### HTTP Tool Responses (TAHMÄ°N)
```
Query 1 (Thread Correlation): ~2K tokens (aggregated counts)
Query 2 (Service Error Distribution): ~3K tokens (rate metrics)
Query 3 (Cascade Timeline): ~150,000 tokens â† PROBLEM! ğŸ”´
Query 4 (Request ID Correlation): ~100,000 tokens â† PROBLEM! ğŸ”´
Query 5 (Error Pattern): ~8K tokens (topk 20)

Total: 5.25K + 2K + 3K + 150K + 100K + 8K = 268,250 tokens
```

**GerÃ§ek hata**: 263,495 tokens â†’ **TAHMÄ°NÄ°MÄ°Z DOÄRU!** âœ…

---

## Ã‡Ã¶zÃ¼m Stratejisi

### Option 1: AgentTools.txt Query Limits (Ã–NERÄ°LEN)

Query 3 ve 4'e **topk + time window** ekle:

#### Query 3: Cascade Timeline (FIX)
```logql
topk(100,  â† LIMIT: Top 100 log
  {namespace=~"12_namespaces"}
  | json
  | __error__=""
  | log_level=~"ERROR|FATAL|WARN"
  | line_format "{{.timestamp}}|{{.namespace}}|{{.service_name}}|{{.process_thread_name}}|{{.error_type}}|{{.message}}"
)
```

**Token reduction**: 150K â†’ ~10K tokens (85% azalma)

#### Query 4: Request ID Correlation (FIX)
```logql
topk(50,  â† LIMIT: Top 50 log
  {namespace=~"12_namespaces"}
  |~ "trace_id|traceID|correlation_id"
  |~ "error"
)
```

**Token reduction**: 100K â†’ ~5K tokens (95% azalma)

**Toplam token after fix**: 5.25K + 2K + 3K + 10K + 5K + 8K = **33,250 tokens** âœ…

---

### Option 2: Node 9 Further Reduction (EK OPTÄ°MÄ°ZASYON)

EÄŸer hala token problemi olursa, Node 9'dan `serviceDependencies` objesini kÄ±salt:

**Mevcut**:
```json
"serviceDependencies": {
  "raw": { /* 28 services Ã— 5 deps = 140 entries */ },
  "reverse": { /* 28 services Ã— deps = 140 entries */ },
  "criticality": { /* 28 services Ã— 5 fields = 140 entries */ },
  "serviceGroups": { /* 7 groups */ },
  "metadata": { /* summary */ }
}
```

**Size**: ~15KB JSON

**Optimized**:
```json
"serviceDependencies": {
  "critical_services": ["bstp-pcm-product-catalog", "ntf-engine-service", ...],  // Top 5 only
  "service_groups": { "notification": [...], "customer": [...] },
  "metadata": { "totalServices": 28 }
  // Remove: raw, reverse, full criticality
}
```

**Size**: ~2KB JSON â†’ **13KB savings** â†’ ~3,250 token reduction

---

## Implementation Priority

### ğŸ”´ CRITICAL (YapÄ±lmalÄ±)

1. **AgentTools.txt Query 3**: Add `topk(100, ...)`
2. **AgentTools.txt Query 4**: Add `topk(50, ...)`

**Estimated impact**: 263K â†’ 33K tokens (87% reduction) âœ…

### ğŸŸ¡ MEDIUM (Ä°steÄŸe baÄŸlÄ±)

3. **Node 9 serviceDependencies**: Simplify to top critical services only

**Additional impact**: -3K tokens

---

## Test Plan

### Before Fix
```bash
# Run workflow, expect error:
# "263,495 tokens (106% over limit)"
```

### After Fix (AgentTools.txt)
```bash
# Update AgentTools.txt with topk limits
# Run workflow, expect:
# Stage 2 completes successfully
# Estimated tokens: ~33K (74% under limit)
```

### Validation
```javascript
// Node 10 console log should show:
console.log("=== STAGE 2 PATTERN ANALYSIS ===");
console.log("Tools executed:", aiOutput.tools_executed);
console.log("Cascade timeline entries:", aiOutput.cascade_timeline?.length || 0);  // Should be â‰¤100
console.log("Request correlations:", aiOutput.request_correlations?.length || 0);  // Should be â‰¤50
```

---

## Files to Modify

1. **LokiFlow/AgentTools.txt** (lines 44-48):
   - Query 3: Wrap with `topk(100, ...)`
   - Query 4: Wrap with `topk(50, ...)`

2. **(Optional) LokiFlow/LokiNodes/9. Pass Time Context to Stage 2.js**:
   - Simplify serviceDependencies to critical services only

---

## Alternative: Increase Limits (NOT RECOMMENDED)

EÄŸer log kalitesi dÃ¼ÅŸerse, topk limitlerini artÄ±r:
- Query 3: `topk(100, ...)` â†’ `topk(200, ...)`
- Query 4: `topk(50, ...)` â†’ `topk(100, ...)`

**Token impact**: 33K â†’ 48K (still under 128K limit)

---

## Summary

**Root Cause**: AgentTools.txt'de Query 3 ve 4'te time range/limit yok â†’ 250K+ raw log dÃ¶nÃ¼yor

**Solution**: topk(100) ve topk(50) ekleyerek log sayÄ±sÄ±nÄ± sÄ±nÄ±rla

**Impact**: 263K â†’ 33K tokens (87% reduction)

**Risk**: DÃœÅÃœK - topk yine de en Ã¶nemli loglarÄ± dÃ¶nÃ¼yor, cascade ve correlation analizi iÃ§in yeterli
