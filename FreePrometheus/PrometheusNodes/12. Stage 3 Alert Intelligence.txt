# Stage 3: Alert Intelligence & Correlation - CONTEXT AWARE

Analyze alerts and correlate with Stage 2 findings.

## üïê TIME PARAMETERS:
Use these EXACT timestamps from context:
- Start Time: {{ $json._context.initialParams.startTime }}
- End Time: {{ $json._context.initialParams.endTime }}

IMPORTANT:
- These are Unix timestamps in seconds
- Convert for display: new Date(timestamp * 1000).toISOString()
- DO NOT use mock dates like "2024-06-15"
- For current timestamp, use new Date().toISOString()

## üìã CONTEXT INFORMATION:
- Context ID: {{ $json._context.contextId }}
- Stage 2 Root Cause: {{ $json.stage2Data.root_cause.issue }}
- Affected Services: {{ JSON.stringify($json.stage2Data.correlation_matrix.affected_services) }}

## üîß TOOL RESPONSE HANDLING:

### For Active Alerts Details tool:
The tool uses this query to get Kubernetes-related alerts:
ALERTS{alertstate="firing",alertname=~"Kube.*|Container.*|Pod.*|Node.*"}

The tool will return alerts with these labels:
- alertname: Name of the alert (e.g., KubePodCrashLooping, KubeNodeNotReady)
- alertstate: Will be "firing" 
- severity: Alert severity (critical, warning, info) if defined in the alert rule
- namespace: Kubernetes namespace where the issue is occurring
- pod: Pod name if the alert is pod-related
- node: Node name if the alert is node-related
- container: Container name if applicable

Note: Summary and description are defined in the alert rules, not in the metrics. If you need detailed descriptions, refer to the alert names:
- KubePodCrashLooping: Pod is restarting frequently
- KubeNodeNotReady: Node is not in ready state
- KubeDeploymentReplicasMismatch: Deployment has incorrect replica count
- KubeContainerWaiting: Container is in waiting state
- KubePodNotReady: Pod is not ready to serve traffic

When analyzing alerts:
1. Count of each alert type
2. Which namespaces/pods/nodes are affected
3. Severity distribution
4. How long alerts have been firing (if timestamp available)

### For Alert History 24h tool:
- Call without parameters
- Returns alert history for the last 24 hours
- Use to identify recurring patterns

### For SLO Status Check Tools:
**CRITICAL**: SLO tools may return "NaN", empty results, or errors. Handle these cases:
- If result is "NaN" ‚Üí assume 100% (no issues detected)
- If result is empty array ‚Üí assume 100% (no metrics to check)
- If result has error ‚Üí assume 100% and note in debug
- If result has value ‚Üí use the numeric value

Example handling:
```javascript
const sloValue = toolResponse.data?.result?.[0]?.value?.[1];
if (!sloValue || sloValue === "NaN" || sloValue === "") {
  return "100"; // No issues detected
}
return sloValue;
üìä COMPOSITE SLO CALCULATION:
If multiple SLO tools are available, calculate a weighted composite:

Call each SLO tool and collect results:

Pod Ready SLO ‚Üí weight: 30%
Container Running SLO ‚Üí weight: 20%
Node Ready SLO ‚Üí weight: 25%
Pod Restart Rate SLO ‚Üí weight: 15%
Deployment Health ‚Üí weight: 10%


For each SLO result:

If "NaN" or empty ‚Üí use 100
If error ‚Üí use 100 and note in debug
Otherwise use actual numeric value


Calculate composite:
composite = (podReady * 0.3) + (containerRunning * 0.2) + (nodeReady * 0.25) + (restartRate * 0.15) + (deploymentHealth * 0.1)
Interpret composite score:



= 99.9 ‚Üí "green" (SLO Met)


99.0-99.9 ‚Üí "yellow" (SLO Warning)
< 99.0 ‚Üí "red" (SLO Violation)



Common Kubernetes Alert Descriptions:
const alertDescriptions = {
"KubePodCrashLooping": {
summary: "Pod is crash looping",
description: "Pod has restarted more than 5 times in the last 10 minutes",
severity: "critical"
},
"KubeNodeNotReady": {
summary: "Node is not ready",
description: "Node has been unready for more than 5 minutes",
severity: "critical"
},
"KubeDeploymentReplicasMismatch": {
summary: "Deployment replica mismatch",
description: "Deployment has not matched the expected number of replicas",
severity: "warning"
},
"KubeContainerWaiting": {
summary: "Container waiting",
description: "Container has been in waiting state for more than 1 hour",
severity: "warning"
},
"KubePodNotReady": {
summary: "Pod not ready",
description: "Pod has been in a non-ready state for more than 5 minutes",
severity: "warning"
},
"KubeHpaMaxedOut": {
summary: "HPA maxed out",
description: "HPA has been at max replicas for more than 15 minutes",
severity: "warning"
}
};

## üéØ DECISION LOGIC - HOW TO SET proceed_to_stage4:

**Set proceed_to_stage4 = true IF ANY OF THESE CONDITIONS:**
1. Active alerts count > 0 (any firing alerts need automated diagnosis)
2. SLO violation detected (availability_slo.status = "red" OR current < 99.0%)
3. High-severity alerts present (Critical or Blocker severity from KB enrichment)
4. Alert storm detected (alert_patterns.storm_detection.detected = true)
5. Stage 2 identified issues but root_cause.confidence < 0.7
6. Multiple alert groups indicating complex cascading failures

**Set proceed_to_stage4 = false ONLY IF ALL CONDITIONS MET:**
1. No active alerts (active_alerts = [] OR all alerts resolved)
   AND
2. SLO status = "green" (availability_slo.current >= 99.9%)
   AND
3. No recurring alert patterns (alert_patterns.recurring = [])
   AND
4. auto_remediation_approved = true (all Low/Medium severity KB matches)
   AND
5. Stage 2 found no issues OR Stage 2 root_cause.confidence >= 0.8

**CRITICAL**: Default to true if uncertain. Better to run diagnosis than miss critical issues.

## üìä ALERT CORRELATION CONFIDENCE CALCULATION:

Calculate overall alert correlation quality (0.0 - 1.0) as SUM of these factors:

**1. Alert-to-Stage2 Correlation (+0.3):**
- Alert labels match Stage 2 root_cause.component (e.g., same pod/namespace) = +0.3
- Alerts in same namespace as Stage 2 affected_services = +0.2
- Alerts related to Stage 2 findings but different component = +0.1
- No correlation with Stage 2 findings = 0.0

**2. Knowledge Base Coverage (+0.3):**
- All active alerts have KB matches (100% coverage) = +0.3
- 75%-99% alerts have KB matches = +0.2
- 50%-74% alerts have KB matches = +0.1
- <50% KB coverage = +0.05
- No KB matches = 0.0

**3. Alert Group Quality (+0.2):**
- Clear root alert identified with 3+ related alerts (strong correlation) = +0.2
- Root alert with 1-2 related alerts (moderate correlation) = +0.1
- No alert grouping possible (isolated alerts) = 0.0

**4. SLO Impact Clarity (+0.2):**
- Composite SLO available with 4+ components = +0.2
- Composite SLO with 2-3 components = +0.1
- Single SLO component = +0.05
- No SLO data (all NaN/empty) = 0.0

**EXAMPLES:**
- 5 alerts, all have KB matches (+0.3) + Alerts match Stage 2 pod (+0.3) + Root alert with 4 related (+0.2) + Composite SLO with 5 components (+0.2) = **Confidence: 1.0** (perfect correlation)
- 2 alerts, 1 KB match (+0.1) + Different namespace than Stage 2 (+0.0) + No grouping (+0.0) + Single SLO (+0.05) = **Confidence: 0.15** (weak correlation, proceed to Stage 4)

## üîß TOOL PARAMETERS:

**IMPORTANT**: SLO tools automatically use multi-namespace filtering via `$json.namespaceRegex`.
You do NOT need to pass namespace parameters when calling tools.

**Optional service filtering**: If Stage 2 identified specific affected services, tools can filter by service name.

Tools will automatically query across ALL 12 production namespaces:
- bstp-cms-global-production, bstp-cms-prod-v3
- em-global-prod-3pp, em-global-prod-eom, em-global-prod-flowe, em-global-prod
- em-prod-3pp, em-prod-eom, em-prod-flowe, em-prod
- etiyamobile-production, etiyamobile-prod

When calling SLO tools, simply call them - no namespace parameter needed:
- Pod Ready SLO ‚Üí Checks all namespaces automatically
- Container Running SLO ‚Üí Checks all namespaces automatically
- Pod Restart Rate SLO ‚Üí Checks all namespaces automatically
- Deployment Replica Health ‚Üí Checks all namespaces automatically

Service filtering (optional) comes from Stage 2 data:
- Service name: {{ $json.stage2Data?.correlation_matrix?.affected_services?.[0] || 'none detected' }}

üö® CRITICAL OUTPUT REQUIREMENT:
RETURN ONLY VALID JSON - NO MARKDOWN, NO CODE BLOCKS
DO NOT WRAP YOUR RESPONSE IN json TAGS
HANDLE ALL EMPTY/NULL RESPONSES GRACEFULLY
Return ONLY this JSON structure:
{
"stage": "alert_intelligence",
"active_alerts": [
{
"name": "<actual alert name from tool response or 'No alerts'>",
"severity": "<actual severity from tool response or 'unknown'>",
"count": <actual count from tool response or 0>,
"duration": "<calculate from current time minus alert start time or 'unknown'>",
"labels": <actual labels object from tool response or {}>,
"annotations": <actual annotations from tool response if available or {}>
}
],
"alert_groups": [
{
"root_alert": "<identify main alert based on correlation or 'none'>",
"related_alerts": [<list other related alerts or empty array>],
"correlation_score": <calculate 0.0-1.0 based on shared labels or 0>,
"shared_labels": <identify common labels between alerts or {}>
}
],
"knowledge_base_matches": [
{
"alert": "<alert name or 'none'>",
"kb_entry": {
"root_causes": ["Based on alert type and previous incidents"],
"diagnostic_commands": ["kubectl describe", "kubectl logs"],
"immediate_actions": ["Check pod status", "Review recent changes"],
"long_term_solutions": ["Update resource limits", "Fix application code"]
},
"applicability_score": <0.0-1.0 based on match or 0>
}
],
"alert_patterns": {
"recurring": [],
"storm_detection": {
"detected": false,
"alert_count": <count of alerts or 0>,
"time_window": "5m",
"likely_root": "<identify if there's a common cause or null>"
}
},
"slo_impact": {
"availability_slo": {
"target": "99.9%",
"current": "<calculated SLO value or '100'>%",
"error_budget_used": "<calculated percentage or '0'>%",
"time_remaining": "<based on burn rate or '30d'>",
"components": {
"deployment_health": "<if single SLO tool used, put value here or '100'>%"
},
"status": "<green|yellow|red based on current vs target>"
},
"affected_slis": [<list SLIs below target or empty array>]
},
"recommended_alert_actions": [
{
"alert": "<alert name or 'none'>",
"action": "<specific remediation action or 'Monitor'>",
"confidence": <0.0-1.0 or 0>,
"risk": "<low|medium|high>",
"command": "<kubectl or other command or null>"
}
],
"correlation_confidence": <calculate 0.0-1.0 using formula above>,
"proceed_to_stage4": <true if alerts need investigation, false otherwise>,
"auto_remediation_approved": <true only for low-risk known issues>,
"_context": <copy the exact _context object from input>,
"_debug": {
"nodeType": "Stage 3: Alert Intelligence",
"processedAt": "<current ISO timestamp from new Date().toISOString()>",
"contextId": "<copy from input _context.contextId>",
"contextPreserved": true,
"receivedFromStage": "Fix Stage 2 Context",
"stageSequence": <take input _debug.stageSequence array and add "Stage 3: Alert Intelligence">,
"timeRangeUsed": {
"start": <copy from input _context.initialParams.startTime>,
"end": <copy from input _context.initialParams.endTime>
},
"sloToolErrors": []
}
}
üìù IMPORTANT NOTES:

If no alerts are found, return empty arrays but still complete the analysis
Use actual values from tool responses, don't make up data
For _context: Copy exactly as received in input
For timestamps: Use actual current time, not placeholders
Use knowledge from alertDescriptions to enrich alert information
ALWAYS handle NaN and empty responses gracefully
Default to safe values when data is missing