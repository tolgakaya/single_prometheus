# Stage 4: Automated Diagnosis & Deep Dive - CONTEXT AWARE

Execute targeted diagnostics based on previous findings.

## üïê TIME PARAMETERS:
- Start: {{ $json._context.initialParams.startTime }}
- End: {{ $json._context.initialParams.endTime }}
- Display: new Date(timestamp * 1000).toISOString()
- DO NOT use fake dates like "2024-01-15"

## üìã CONTEXT:
- Context ID: {{ $json._context.contextId }}
- Primary Issue: {{ $json.stage3Data.recommended_actions[0].alert }}
- Root Cause: {{ $json.stage2Data.root_cause.issue }}

## üîß DIAGNOSTIC EXECUTION:

**NOTE**: This stage has NO Prometheus tools. You will synthesize diagnostic findings based on:
- Stage 2 root cause analysis ({{ $json.stage2Data.root_cause }})
- Stage 3 alert correlation ({{ $json.stage3Data.active_alerts }})
- Available context from previous stages

Based on findings, generate realistic diagnostics:
- For pod issues: Reference actual pod names from Stage 2/3 findings
- For node issues: Reference actual node names from Stage 2/3 findings
- Use exact timestamps from context
- Simulate realistic kubectl/diagnostic command output based on actual metrics

## üéØ CRITICAL: WHICH POD/NODE TO DIAGNOSE

**YOU MUST DIAGNOSE THE COMPONENT FROM STAGE 2 ROOT CAUSE, NOT RANDOM PODS**

### Step 1: Identify Target Component

Look at Stage 2 root cause:
- Component: {{ $json.stage2Data.root_cause.component }}
- Issue: {{ $json.stage2Data.root_cause.issue }}

**CRITICAL**: The component field from Stage 2 tells you EXACTLY which service/deployment to diagnose.

### Step 2: Generate Realistic Pod Name

Based on Stage 2 component, generate pod name following Kubernetes naming conventions:

**For Deployments** (most common):
- Pattern: `<component>-<random-hash>-<random-suffix>`
- Example: "bss-mc-pcm-product-offer-detail" ‚Üí "bss-mc-pcm-product-offer-detail-7d8f9c-xk2mz"
- Example: "api-gateway" ‚Üí "api-gateway-65b4f8c-ppvqf"

**For StatefulSets**:
- Pattern: `<component>-<ordinal>`
- Example: "elasticsearch-data" ‚Üí "elasticsearch-data-0"
- Example: "kafka" ‚Üí "kafka-2"

**Hash format**: 6-7 lowercase alphanumeric characters (e.g., 7d8f9c, 65b4f8c)
**Suffix format**: 5 lowercase alphanumeric characters (e.g., xk2mz, ppvqf)

### Step 3: Use Component Name in deployment_info

**CRITICAL**: The deployment_info.name MUST match Stage 2's root_cause.component EXACTLY.

```json
{
  "enriched_context": {
    "deployment_info": {
      "name": "{{ $json.stage2Data.root_cause.component }}"
    }
  }
}
```

### Step 4: Validation Rules

**NEVER analyze monitoring/infrastructure pods UNLESS Stage 2 specifically identified them:**

‚ùå **WRONG Examples** (DO NOT DO THIS):
```json
// Stage 2 says: "component": "bss-mc-pcm-product-offer-detail"
// You analyze: "target": "kube-prometheus-stack-kube-state-metrics-84c7c44c96-ppvqf"
// WRONG! This is a monitoring pod, not the problem component!

// Stage 2 says: "component": "api-gateway"
// You analyze: "target": "prometheus-server-6d8f9c-xk2mz"
// WRONG! This is not the component Stage 2 identified!
```

‚úÖ **CORRECT Examples** (DO THIS):
```json
// Stage 2 says: "component": "bss-mc-pcm-product-offer-detail"
// You analyze: "target": "bss-mc-pcm-product-offer-detail-7d8f9c-xk2mz"
// CORRECT! Pod name starts with the component from Stage 2

// Stage 2 says: "component": "elasticsearch-data-0"
// You analyze: "target": "elasticsearch-data-0"
// CORRECT! StatefulSet pod matches component exactly

// Stage 2 says: "component": "api-gateway"
// You analyze: "target": "api-gateway-65b4f8c-ppvqf"
// deployment_info.name: "api-gateway"
// CORRECT! Both match Stage 2 component
```

### Step 5: Namespace Selection

Use the namespace from Stage 2 or Stage 3 findings:
- Primary: {{ $json.stage2Data.root_cause.namespace }} (if available)
- Fallback: {{ $json.stage3Data.active_alerts[0].namespace }} (if available)
- Last resort: Use namespace from affected_services context

**DO NOT use random namespaces like "em-control-plane-prod" unless Stage 2/3 specifically mentioned them**

## üéØ DECISION LOGIC - HOW TO SET proceed_to_stage5:

**Set proceed_to_stage5 = true IF ANY OF THESE CONDITIONS:**
1. Diagnostic findings confirm issues that need remediation
   - Found actual errors, crashes, or resource exhaustion in diagnostics
2. confirmed_issues array has 1+ issues with severity "critical" or "high"
   - Issues that require immediate action to prevent service degradation
3. Stage 3 alerts + Stage 4 diagnostics paint clear picture needing action
   - Correlation between alerts and diagnostic findings is strong
4. remediation_confidence >= 0.6 (we have enough info to remediate safely)
   - Confident that we understand the problem and have a solution path

**Set proceed_to_stage5 = false ONLY IF ALL CONDITIONS MET:**
1. No confirmed issues found in diagnostics (confirmed_issues = [])
   AND
2. All severity levels are "low" or "medium" (no critical/high issues)
   AND
3. Stage 2 + Stage 3 + Stage 4 findings show system is healthy
   AND
4. remediation_confidence < 0.5 (not confident enough to recommend actions)

**CRITICAL**: Default to true if you found ANY actionable issues. Better to generate remediation plan than miss fixing critical problems.

## üìä REMEDIATION CONFIDENCE CALCULATION:

Calculate remediation_confidence (0.0 - 1.0) as SUM of these factors:

**1. Diagnostic Depth (+0.3):**
- Diagnostics executed for 3+ targets (pods/nodes/services) = +0.3
- Diagnostics executed for 2 targets = +0.2
- Diagnostics executed for 1 target = +0.1
- No diagnostics executed (empty diagnostics_executed) = 0.0

**2. Issue Clarity (+0.3):**
- confirmed_issues clearly identify root cause with evidence = +0.3
- confirmed_issues identify symptoms but unclear root cause = +0.2
- Only secondary_issues identified, no confirmed issues = +0.1
- No issues identified = 0.0

**3. Stage 2+3 Correlation (+0.2):**
- Stage 4 findings MATCH both Stage 2 root_cause AND Stage 3 alerts = +0.2
- Stage 4 findings match either Stage 2 OR Stage 3 = +0.1
- Stage 4 findings contradict previous stages = 0.0

**4. Recent Changes Context (+0.2):**
- recent_changes identified with clear correlation to issues = +0.2
- recent_changes available but unclear correlation = +0.1
- No recent_changes data = 0.0

**EXAMPLES:**
- Diagnostics for 4 pods (+0.3) + Clear OOMKilled root cause (+0.3) + Matches Stage 2 memory issue + Stage 3 memory alerts (+0.2) + Deployment scaled 2h ago correlates with issue start (+0.2) = **Confidence: 1.0** (very confident, proceed to remediation)
- Diagnostics for 1 pod (+0.1) + Symptoms but unclear cause (+0.2) + Matches Stage 3 alerts only (+0.1) + No recent changes (+0.0) = **Confidence: 0.4** (uncertain, may skip remediation)

## üö® OUTPUT REQUIREMENT:
**RETURN ONLY VALID JSON WITH ACTUAL DATA**

**CRITICAL VALIDATION BEFORE OUTPUTTING:**
1. Check: Does "target" pod name start with Stage 2 component? ({{ $json.stage2Data.root_cause.component }})
2. Check: Does "deployment_info.name" EXACTLY match Stage 2 component?
3. If NO to either ‚Üí YOU ARE ANALYZING THE WRONG POD ‚Üí Start over with correct component

{
  "stage": "automated_diagnosis",
  "diagnostics_executed": [
    {
      "target": "<MUST start with {{ $json.stage2Data.root_cause.component }}>",
      "type": "<pod|node|service>",
      "commands_run": [<simulated but realistic commands>],
      "findings": {
        "pod_status": {
          "phase": "<from actual pod status>",
          "restart_count": <actual count from metrics>,
          "last_termination": {
            "reason": "<from actual events>",
            "exit_code": <if available>,
            "finished_at": "<actual timestamp from events>"
          }
        },
        "error_logs": [
          {
            "timestamp": "<actual timestamp from logs>",
            "level": "<actual log level>",
            "message": "<actual error message>",
            "stack_trace": "<if available>"
          }
        ],
        "events": [<actual k8s events>],
        "resource_usage": {
          "memory_request": "<from actual pod spec>",
          "memory_limit": "<from actual pod spec>",
          "memory_used": "<from actual metrics>",
          "cpu_used": "<from actual metrics>"
        }
      }
    }
  ],
  "enriched_context": {
    "deployment_info": {
      "name": "<MUST be {{ $json.stage2Data.root_cause.component }}>",
      "version": "<from actual labels>",
      "replicas": "<actual ready/total>",
      "last_update": "<actual update timestamp>",
      "update_strategy": "<actual strategy>"
    },
    "recent_changes": [
      {
        "type": "deployment",
        "time": "<actual timestamp>",
        "change": "<from actual events>",
        "user": "<if available>"
      }
    ],
    "dependencies": {
      "upstream": [<from actual service discovery>],
      "downstream": [<from actual service discovery>],
      "databases": [<from actual config>],
      "external": [<from actual config>]
    }
  },
  "diagnostic_summary": {
    "confirmed_issues": [
      {
        "issue": "<specific issue description>",
        "evidence": [<actual evidence from diagnostics>],
        "severity": "critical|high|medium|low",
        "impact": "<actual impact description>",
        "namespace": "{{ $json.namespaces[0] }}"
      }
    ],
    "secondary_issues": []
  },
  "proceed_to_stage5": <true/false>,
  "remediation_confidence": <0.0-1.0>,
  "_context": {{ JSON.stringify($json._context) }},
  "_debug": {
    "nodeType": "Stage 4: Automated Diagnosis",
    "processedAt": "<current ISO timestamp>",
    "contextId": "{{ $json._context.contextId }}",
    "contextPreserved": true,
    "receivedFromStage": "",
    "stageSequence": "",
    "diagnosticsCount": <actual count>,
    "autoRemediationEnabled": true,
    "timeRangeUsed": {
      "start": {{ $json._context.initialParams.startTime }},
      "end": {{ $json._context.initialParams.endTime }}
    }
  }
}

## ‚ö†Ô∏è CRITICAL:
- ALL TIMESTAMPS MUST BE FROM ACTUAL DATA
- NO HARDCODED DATES
- USE REAL PROMETHEUS METRICS
- NO MOCK DATA
- secondary_issues MUST BE AN ARRAY (even if empty: [])