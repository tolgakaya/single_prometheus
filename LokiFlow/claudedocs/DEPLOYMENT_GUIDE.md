# LokiFlow Deployment Guide

## Overview

This guide covers deployment of all fixes and enhancements implemented during the comprehensive LokiFlow analysis and implementation session.

---

## What Was Fixed

### Critical Fixes (5/5 Completed)
1. âœ… **analysisId Preservation** - Prevents correlation tracking breaks
2. âœ… **Standardized Data Structure** - Consistent schema across all nodes
3. âœ… **Validation Nodes** - Data integrity checks at each stage
4. âœ… **Anti-Mock-Data Instructions** - Prevents AI from fabricating data
5. âœ… **Service Dependencies Externalization** - ConfigMap for easy updates

### High Priority Fixes (5/5 Completed)
1. âœ… **Stage 1 Wait Time Reduction** - 60% faster execution (30s â†’ 12s)
2. âœ… **Anomaly Score Validation** - Ensures real calculations
3. âœ… **Tool Execution Validation** - Verifies AI executes required tools
4. âœ… **Context Preservation** - Simplified node logic
5. âœ… **Debug Logging** - Enhanced troubleshooting

---

## Deployment Steps

### Phase 1: Node Updates (Zero Downtime)

All node updates are backward compatible and can be deployed without downtime.

#### 1.1 Update JavaScript Nodes in n8n

Import updated nodes in this order:

```
Priority 1 (Core Structure):
â†’ 2. Time Range Handler.js (initializes standardized structure)
â†’ 3. Set Workflow Variables.js (reads standardized structure)
â†’ 4. Service Dependency Loader.js (updates context.serviceDependencies)

Priority 2 (Stage Flow):
â†’ 6. Pass Context to Anomaly Stage.js (Stage 1 â†’ Anomaly)
â†’ 8. Merge Anomaly Results.js (merges TRUE/FALSE branches)
â†’ 11. Preserve Context After Stage 2.js (Stage 2 â†’ Stage 3)

Priority 3 (Output):
â†’ 12. Cascade Failure Detector.js (enrichments)
â†’ 15. Combine All Stages.js (combines all results)
â†’ 16. Format Final Output.js (JIRA ticket generation)
```

**How to update each node**:
1. Open n8n workflow editor
2. Click on the node to edit
3. Replace JavaScript code with updated version
4. Click "Execute Node" to test
5. Save workflow

**Rollback plan**: Keep backup of old workflow version. If issues occur, revert to previous workflow version from n8n version history.

#### 1.2 Update AI Agent Prompts

Update prompts in this order:

```
Stage 1: 5. Stage 1 Quick Health Check.txt
         - Anti-mock-data instructions (lines 11-22)
         - Wait time reduction (line 118: 10s â†’ 3s)
         - Tool execution validation (lines 115-131)

Stage 1.5: 7. Anomaly Detection.txt
           - Anti-mock-data instructions (lines 12-30)
           - Anomaly score validation (lines 20-24)
           - Tool execution validation (lines 125-138)

Stage 2: 10. Stage 2 Pattern Analysis.txt
         - Anti-mock-data instructions (lines 8-19)
         - Wait time reduction (line 93: 10s â†’ 3s)
         - Tool execution validation (lines 77-90)
```

**How to update prompts**:
1. Copy updated prompt content
2. Open n8n workflow
3. Find "Stage 1 AI Agent" node (or Stage 2, Anomaly)
4. Paste new prompt into agent configuration
5. Test with sample data
6. Save workflow

**Rollback plan**: Keep old prompts in `LokiNodes/backup/` directory. Revert if AI behavior degrades.

### Phase 2: Validation Nodes (Recommended)

Add validation nodes between stages to catch data integrity issues early.

#### 2.1 Create Validation Nodes

Create 4 new Code nodes in n8n:

```
After Stage 1:
â†’ 5.5 Validate After Stage 1.js
   - Insert between "Stage 1 AI Agent" and "Check Anomaly Decision"
   - Validates metadata, context, stageResults.stage1

After Anomaly:
â†’ 7.5 Validate After Anomaly Detection.js
   - Insert after "Merge Anomaly Results"
   - Validates stageResults.stage1_5_anomaly

After Stage 2:
â†’ 13. Validate After Stage 2.js
   - Insert after "Preserve Context After Stage 2"
   - Validates stageResults.stage2, enrichments.cascadeAnalysis

After Stage 3:
â†’ 14.5 Validate After Stage 3.js
   - Insert after "Combine All Stages"
   - Validates all stages, checks completion
```

**Workflow integration**:
```
Stage 1 AI Agent
  â†“
âœ… [NEW] 5.5 Validate After Stage 1
  â†“
Check Anomaly Decision
  â†“
Anomaly Detection AI Agent
  â†“
Merge Anomaly Results
  â†“
âœ… [NEW] 7.5 Validate After Anomaly Detection
  â†“
Stage 2 AI Agent
  â†“
Preserve Context After Stage 2
  â†“
âœ… [NEW] 13. Validate After Stage 2
  â†“
Cascade Failure Detector
  â†“
Stage 3 AI Agent
  â†“
Combine All Stages
  â†“
âœ… [NEW] 14.5 Validate After Stage 3
  â†“
Format Final Output
```

**Error handling**: All validation nodes throw errors if critical validation fails. Configure n8n error workflow to:
1. Log validation error details
2. Send alert to platform team
3. Include analysisId for correlation

#### 2.2 Testing Validation Nodes

Test each validation node:

```bash
# Test 1: Valid data (should pass)
- Run workflow with normal alert
- Verify all 4 validation nodes pass
- Check no errors in n8n logs

# Test 2: Missing analysisId (should fail)
- Manually remove analysisId from Node 2 output
- Verify validation node throws error
- Check error message includes "Missing analysisId"

# Test 3: Missing stage result (should fail)
- Simulate Stage 2 not executing
- Verify validation node after Stage 2 throws error
- Check error mentions "Missing stageResults.stage2"
```

### Phase 3: Kubernetes ConfigMap (Optional but Recommended)

Externalize service dependencies to ConfigMap for easier updates.

#### 3.1 Deploy ConfigMap

```bash
# Deploy to monitoring namespace
kubectl apply -f kubernetes/service-dependencies-configmap.yaml

# Verify deployment
kubectl get configmap lokiflow-service-dependencies -n monitoring
kubectl describe configmap lokiflow-service-dependencies -n monitoring

# View contents
kubectl get configmap lokiflow-service-dependencies -n monitoring -o yaml
```

#### 3.2 Update Node 4 to Load from ConfigMap (Optional)

**Current**: Node 4 has hardcoded service dependencies (90 services)

**Future enhancement**: Update Node 4 to load from ConfigMap:

```javascript
// Option A: Mount ConfigMap as volume in n8n pod
const fs = require('fs');
const dependencies = JSON.parse(
  fs.readFileSync('/config/service-dependencies/dependencies.json', 'utf8')
);

// Option B: Use Kubernetes API from n8n
const k8s = require('@kubernetes/client-node');
const kc = new k8s.KubeConfig();
kc.loadFromDefault();
const k8sApi = kc.makeApiClient(k8s.CoreV1Api);
const configMap = await k8sApi.readNamespacedConfigMap(
  'lokiflow-service-dependencies',
  'monitoring'
);
const dependencies = JSON.parse(configMap.body.data['dependencies.json']);
```

**Deployment**:
1. Update n8n deployment to mount ConfigMap (if using Option A)
2. Update Node 4 JavaScript code
3. Test with sample data
4. Verify same behavior as hardcoded version
5. Remove hardcoded dependencies after verification

**Benefits**:
- Update dependencies without redeploying workflow
- Centralized dependency management
- Version control for dependency changes
- Easier to sync with actual production services

---

## Testing Strategy

### Unit Testing (Per Node)

Test each updated node individually:

```
Node 2 - Time Range Handler:
âœ“ Verify creates metadata section with analysisId
âœ“ Verify creates context section with timeRange
âœ“ Verify creates empty stageResults section
âœ“ Verify legacy fields exist for backward compatibility

Node 3 - Set Workflow Variables:
âœ“ Verify reads from metadata.forceDeepAnalysis
âœ“ Verify falls back to legacy forceDeepAnalysis
âœ“ Verify preserves entire standardized structure

Node 4 - Service Dependency Loader:
âœ“ Verify updates context.serviceDependencies
âœ“ Verify legacy serviceDependencies at root level
âœ“ Verify criticality calculation correct

... (test all 9 updated nodes)
```

### Integration Testing (Full Pipeline)

Test complete flow with real alert data:

```
Test Case 1: Normal Alert (no anomaly)
â†’ Trigger: Alert with <0.3 anomaly scores
â†’ Expected: Stage 1 â†’ Anomaly (skip) â†’ Format Output
â†’ Verify: All validations pass, correct JIRA ticket

Test Case 2: Anomaly Detected
â†’ Trigger: Alert with >0.6 anomaly score
â†’ Expected: Stage 1 â†’ Anomaly â†’ Stage 2 â†’ Cascade â†’ Output
â†’ Verify: proceed_to_stage2 = true, patterns detected

Test Case 3: Cascade Failure
â†’ Trigger: Multi-service errors <2s apart
â†’ Expected: Full pipeline to Stage 3
â†’ Verify: Cascade detected, root cause identified, JIRA created

Test Case 4: Mock Data Prevention
â†’ Trigger: Alert with no Loki data (empty results)
â†’ Expected: AI states "No data found", doesn't fabricate
â†’ Verify: Output says "No errors found" instead of example data
```

### Regression Testing

Ensure backward compatibility:

```
âœ“ Legacy fields still accessible (analysisId at root)
âœ“ Old n8n expressions still work ({{$json.analysisId}})
âœ“ JIRA ticket format unchanged
âœ“ Slack notifications unchanged
âœ“ API response structure unchanged
```

### Performance Testing

Verify performance improvements:

```
Before fixes:
- Stage 1 execution: ~30 seconds (3 tools Ã— 10s wait)
- Stage 2 execution: ~40 seconds (4 tools Ã— 10s wait)
- Total pipeline: ~2-3 minutes

After fixes:
- Stage 1 execution: ~12 seconds (3 tools Ã— 3s wait) â†’ 60% faster
- Stage 2 execution: ~16 seconds (4 tools Ã— 3s wait) â†’ 60% faster
- Total pipeline: ~1-1.5 minutes â†’ 40% faster overall

Validation overhead:
- Each validation node: <100ms
- Total validation overhead: <400ms (negligible)
```

---

## Monitoring & Alerts

### Success Metrics

Track these metrics after deployment:

```
Data Quality:
âœ“ analysisId preservation rate: Should be 100%
âœ“ Mock data incidents: Should be 0
âœ“ Validation failures: Should be <1% (only on real issues)
âœ“ Stage result preservation: Should be 100%

Performance:
âœ“ Stage 1 avg execution time: Should be ~12s (down from ~30s)
âœ“ Stage 2 avg execution time: Should be ~16s (down from ~40s)
âœ“ Total pipeline time: Should be ~1-1.5min (down from ~2-3min)

Quality:
âœ“ JIRA tickets with real data: Should be 100%
âœ“ Anomaly scores calculated: Should be 100% when executed
âœ“ Tool execution rate: Should be 100% (all required tools executed)
```

### Alert Configuration

Set up alerts for:

```
CRITICAL Alerts:
â†’ Validation node failure (data integrity issue)
â†’ analysisId regeneration detected
â†’ All anomaly scores = 0.0 (calculation failure)
â†’ Stage result missing (data loss)

WARNING Alerts:
â†’ Tool execution failure (Loki query timeout)
â†’ Stage execution >2x expected time
â†’ Mock data patterns detected in output
â†’ Confidence score <0.3 for all stages
```

### Logging

Enhanced debug logging at each stage:

```
Node 2 (Time Range Handler):
â†’ Log: analysisId created/preserved
â†’ Log: Time range calculated (start, end, duration)

Validation Nodes:
â†’ Log: Validation passed with field counts
â†’ Log: Warnings (non-critical issues)
â†’ Log: Errors (critical failures)

Stage AI Agents:
â†’ Log: Tools executed with result counts
â†’ Log: Anomaly scores calculated
â†’ Log: proceed_to_next_stage decision with reason
```

---

## Rollback Plan

If issues occur after deployment:

### Immediate Rollback (5 minutes)

```
1. Open n8n workflow
2. Click "Workflow Versions" (top right)
3. Select previous version before updates
4. Click "Restore this version"
5. Verify workflow working
```

### Partial Rollback (Node-by-Node)

If only specific node has issues:

```
1. Identify failing node from logs
2. Restore just that node's code
3. Keep other updated nodes
4. Test with sample data
5. Deploy if working
```

### Prompt Rollback

If AI behavior degrades:

```
1. Restore old prompts from LokiNodes/backup/
2. Update AI agent nodes in n8n
3. Test with sample data
4. Compare outputs (old vs new)
5. Deploy working version
```

---

## Known Issues & Limitations

### Current Limitations

1. **Service Dependencies Hardcoded**
   - Still in Node 4 JavaScript (90 services)
   - ConfigMap deployment optional
   - Update Node 4 to use ConfigMap for easier maintenance

2. **No Automated Tests**
   - Manual testing required for each deployment
   - Consider adding n8n workflow tests
   - Set up staging environment for safe testing

3. **Validation Nodes Optional**
   - Not required for operation
   - Recommended for production
   - Add error handling workflow for validation failures

### Breaking Changes

**None** - All updates are backward compatible:
- Legacy fields maintained alongside new structure
- Old n8n expressions continue working
- JIRA ticket format unchanged
- API contracts unchanged

---

## Support & Troubleshooting

### Common Issues

**Issue 1: Validation node fails with "Missing analysisId"**
```
Cause: Node 2 not preserving analysisId
Fix: Verify Node 2 updated with fix at line 191
Check: analysisId: input.analysisId || input.requestId || ...
```

**Issue 2: AI returns template/mock data**
```
Cause: Prompt not updated with anti-mock-data instructions
Fix: Update AI agent prompt with new instructions
Check: Prompt has "ðŸ”§ IMPORTANT: USE ACTUAL DATA FROM TOOLS" section
```

**Issue 3: Stage 1 still slow (~30s)**
```
Cause: Wait time not reduced in prompt
Fix: Update Stage 1 prompt line 118: 10 seconds â†’ 3 seconds
Check: Prompt says "wait for 3 seconds after using a tool"
```

**Issue 4: All anomaly scores = 0.0**
```
Cause: AI not calculating from real tool data
Fix: Verify Anomaly Detection prompt has score validation section
Check: Tools executed array should have 4 tools listed
```

### Debug Checklist

When investigating issues:

```
âœ“ Check n8n execution logs for errors
âœ“ Verify analysisId same across all stages
âœ“ Check stageResults section has all stages
âœ“ Verify tools_executed arrays populated
âœ“ Check confidence_score values > 0
âœ“ Verify JIRA ticket has real data (not examples)
âœ“ Check execution time reasonable (~1-1.5min total)
```

### Contact

For issues or questions:
- Platform Team: platform-team@company.com
- n8n Support: n8n-admin@company.com
- On-call: Check PagerDuty rotation

---

## Version History

**v1.0.0** (2025-12-19)
- Initial deployment of comprehensive fixes
- 5 CRITICAL + 5 HIGH priority fixes implemented
- Standardized data structure across all nodes
- Added 4 validation nodes
- ConfigMap for service dependencies
- 60% performance improvement on Stage 1 & 2
